{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobchang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jacobchang/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dizex/FoodBaseBERT\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Dizex/FoodBaseBERT\")\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-FOOD', 'score': np.float32(0.6692053), 'index': 6, 'word': 'Fresh', 'start': 14, 'end': 19}, {'entity': 'I-FOOD', 'score': np.float32(0.5334641), 'index': 7, 'word': 'olive', 'start': 20, 'end': 25}, {'entity': 'I-FOOD', 'score': np.float32(0.9861605), 'index': 8, 'word': 'p', 'start': 26, 'end': 27}, {'entity': 'I-FOOD', 'score': np.float32(0.9927375), 'index': 9, 'word': '##oke', 'start': 27, 'end': 30}, {'entity': 'I-FOOD', 'score': np.float32(0.97065824), 'index': 10, 'word': 'bowl', 'start': 31, 'end': 35}, {'entity': 'B-FOOD', 'score': np.float32(0.9957469), 'index': 13, 'word': 'ch', 'start': 48, 'end': 50}, {'entity': 'B-FOOD', 'score': np.float32(0.97199684), 'index': 14, 'word': '##ia', 'start': 50, 'end': 52}, {'entity': 'I-FOOD', 'score': np.float32(0.99487525), 'index': 15, 'word': 'seeds', 'start': 53, 'end': 58}]\n"
     ]
    }
   ],
   "source": [
    "example = \"Today's meal: Fresh olive poke bowl topped with chia seeds. Very delicious!\"\n",
    "\n",
    "ner_entity_results = pipe(example)\n",
    "print(ner_entity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d613b032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foods': ['Fresh olive p oke bowl', 'ch', 'ia seeds']}\n"
     ]
    }
   ],
   "source": [
    "foods = []\n",
    "current_food = \"\"\n",
    "\n",
    "for ent in ner_entity_results:\n",
    "    word = ent[\"word\"]\n",
    "    # Handle wordpieces (like ##oke -> oke)\n",
    "    if word.startswith(\"##\"):\n",
    "        word = word[2:]\n",
    "    \n",
    "    if ent[\"entity\"].startswith(\"B-FOOD\"):\n",
    "        if current_food:\n",
    "            foods.append(current_food.strip())\n",
    "        current_food = word + \" \"\n",
    "    elif ent[\"entity\"].startswith(\"I-FOOD\"):\n",
    "        current_food += word + \" \"\n",
    "\n",
    "# Add last entity\n",
    "if current_food:\n",
    "    foods.append(current_food.strip())\n",
    "\n",
    "# Deduplicate and clean\n",
    "foods = [f.replace(\" ##\", \"\").replace(\"  \", \" \").strip() for f in foods]\n",
    "foods = list(dict.fromkeys(foods))  # preserve order\n",
    "\n",
    "# Store in a dictionary\n",
    "food_dict = {\"foods\": foods}\n",
    "print(food_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0f5b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'locations': ['Berlin'], 'dates': [datetime.datetime(2025, 6, 3, 0, 0)]}\n",
      "{'locations': ['MD'], 'dates': []}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from dateparser import parse\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(entry):\n",
    "    doc = nlp(entry)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "    parsed_dates = [parse(d) for d in dates if parse(d)]\n",
    "    return {\"locations\": locations, \"dates\": parsed_dates}\n",
    "\n",
    "print(extract_entities(\"Flew to Berlin on June 3rd\"))\n",
    "print(extract_entities(\"Went to MD, then had tacos\"))\n",
    "# -> {'locations': ['Berlin'], 'dates': [datetime(2024, 6, 3, 0, 0)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9bb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel('data/cleaned/daily_sentences_cleaned.xlsx')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        masked_embeddings = embeddings * mask\n",
    "        summed = torch.sum(masked_embeddings, 1)\n",
    "        counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        mean_pooled = summed / counts\n",
    "        return mean_pooled.cpu().numpy()\n",
    "\n",
    "embeddings = get_embeddings(df['Sentence'].tolist())\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"journal2\")\n",
    "\n",
    "collection.add(\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=df['Sentence'].tolist(),\n",
    "    metadatas=[{'date': str(d)} for d in df['Date']],\n",
    "    ids=[str(i) for i in range(len(df))]\n",
    ")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def ask(question, k=5):\n",
    "    q_emb = get_embeddings([question])\n",
    "    results = collection.query(query_embeddings=q_emb.tolist(), n_results=k)\n",
    "    \n",
    "    docs = results['documents'][0]\n",
    "    metas = results['metadatas'][0]\n",
    "    \n",
    "    # Combine context with dates\n",
    "    combined = \"\\n\".join([f\"{m['date']}: {d}\" for d, m in zip(docs, metas)])\n",
    "    \n",
    "    # Summarize retrieved context\n",
    "    summary = summarizer(\n",
    "        combined,\n",
    "        max_length=130,\n",
    "        min_length=30,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6c1b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Made mongolian beef for lunch, then went into office. Got free taco bell. Then made chicken and dumplings for dinner. Went to gym then got popeyes wings. Then watched bad batch.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What's my favorite food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fd34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MacBook Pro ORG\n",
      "Bose QuietComfort PRODUCT\n",
      "last week DATE\n",
      "Samsung ORG\n",
      "Whirlpool PRODUCT\n",
      "KitchenAid ORG\n",
      "LEGO Star Wars PRODUCT\n",
      "The Apple Watch Ultra 2 ORG\n",
      "Dell ORG\n",
      "XPS 15 PRODUCT\n",
      "Coca-Cola ORG\n",
      "Oreos PRODUCT\n",
      "Nintendo Switch ORG\n",
      "PlayStation 5 PRODUCT\n",
      "the Dyson Airwrap PERSON\n",
      "Tesla Model ORG\n",
      "['Bose QuietComfort', 'Whirlpool', 'LEGO Star Wars', 'XPS 15', 'Oreos', 'PlayStation 5']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained English model (includes NER)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Example text\n",
    "text = '''I ordered a new MacBook Pro and a pair of Bose QuietComfort headphones last week.  \n",
    "The Samsung Galaxy S24 Ultra has an amazing camera.  \n",
    "We replaced our old Whirlpool dishwasher with a KitchenAid one.  \n",
    "He bought LEGO Star Wars sets for his nephew.  \n",
    "The Apple Watch Ultra 2 syncs perfectly with my iPhone.  \n",
    "I used a Canon EOS R8 for the shoot and edited it on a Dell XPS 15.  \n",
    "She picked up a bottle of Coca-Cola and a pack of Oreos from the store.  \n",
    "I played the game on my Nintendo Switch and later tried it on PlayStation 5.  \n",
    "My friend swears by the Dyson Airwrap for her hair.  \n",
    "The Tesla Model Yâ€™s touchscreen feels smoother than before.'''\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities labeled as PRODUCT\n",
    "products = [ent.text for ent in doc.ents if ent.label_ == \"PRODUCT\"]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383276bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = 'Got free taco bell. Then made chicken and dumplings for dinner.'\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities labeled as PRODUCT\n",
    "products = [ent.text for ent in doc.ents if ent.label_ == \"PRODUCT\"]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151833d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Sentence  locations dates\n",
      "0              {'locations': [], 'dates': []}         []    []\n",
      "1              {'locations': [], 'dates': []}         []    []\n",
      "2              {'locations': [], 'dates': []}         []    []\n",
      "3              {'locations': [], 'dates': []}         []    []\n",
      "4              {'locations': [], 'dates': []}         []    []\n",
      "...                                       ...        ...   ...\n",
      "1294  {'locations': ['England'], 'dates': []}  [England]    []\n",
      "1295           {'locations': [], 'dates': []}         []    []\n",
      "1296           {'locations': [], 'dates': []}         []    []\n",
      "1297           {'locations': [], 'dates': []}         []    []\n",
      "1298           {'locations': [], 'dates': []}         []    []\n",
      "\n",
      "[1299 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/cleaned/daily_sentences_cleaned.xlsx')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# Use aggregation_strategy=\"simple\" to merge tokens into full entities\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"locations\": [], \"dates\": []}\n",
    "    entities = nlp(text)\n",
    "    locations = [e[\"word\"] for e in entities if e[\"entity_group\"] in [\"LOC\", \"GPE\"]]\n",
    "    dates = [e[\"word\"] for e in entities if e[\"entity_group\"] == \"DATE\"]\n",
    "    return {\"locations\": locations, \"dates\": dates}\n",
    "\n",
    "df = df[\"Sentence\"].apply(extract_entities)\n",
    "df = pd.concat([df, df.apply(pd.Series)], axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ce9b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##VA', '##aithersburg', '##ayuga', '##el Tower', '##es', '##gmans', '##gt', '##h', '##il', '##jin', '##lum', '##mofo', '##nan', '##oanoke', '##r America', '##ratton', '##richa', '##riott', '##rlin', '##s', '##sitano', '##to', '##town', '##yang', 'Amazon', 'Amma', 'Ammas', 'Anneheim', 'Argentina', 'Arlington', 'Asukasa', 'Baltimore', 'Belluno', 'Belluno National Park', 'Belmont Bay', 'Ben Gongs', 'Berlin', 'Bethlehem', 'Blacksburg', 'Boston', 'Bradley', 'Brandon â€™ s', 'Brazil', 'Bryant Park', 'Busch Gardens', 'C', 'Cali', 'Canada', 'Cancun', 'Carcassone', 'Castle', 'Central Park', 'Chantilly', 'Chantilly Bible Church', 'Charleston', 'Chinatown', 'Chincoteague', 'Church', 'Clarendon', 'Colombia', 'Coronado', 'Cortina', 'Coyote Lake', 'Croatia', 'Culpeper', 'DC', 'Dam', 'Dee', 'Delaware', 'Dontur', 'Dortmund', 'Due South', 'Dulles', 'East', 'Eiff', 'Elise', 'Elises', 'England', 'Europe', 'Fairfax', 'Fburg', 'Fin', 'Florence', 'Flushing', 'Fontana', 'France', 'Fredericksburg', 'Georgetown', 'Ginza', 'Golds', 'GooseChas', 'Gourmeltz', 'Grea', 'Great Wall', 'Harijuku', 'Iceland', 'Independence Hall', 'Irvine', 'Ithaca', 'Japan', 'Joli', 'Jordan', 'Joshua', 'Joshua Tree', 'Kebab', 'Kona', 'Krissas', 'Kroger', 'Kyoto', 'LA', 'La Jolla', 'Lane', 'Leesburg', 'Littles', 'London', 'Long Beach', 'Lot', 'Louvre', 'Luke', 'M', 'MD', 'Manassass', 'Mars', 'Maryland', 'Milan', 'Milan Chinatown', 'Minwoo', 'Monaco', 'Monterrey', 'Montpellier', 'Moss', 'Munich', 'Myrtle', 'NO', 'NOVA', 'NYC', 'Nags Head', 'Naples', 'Nepal', 'Nice', 'Nigeria', 'Nova', 'Oakland', 'Old Town', 'Old Town Road', 'Olive Garden', 'Omomo', 'Orange Square', 'Orobae thai', 'Osaka', 'Osaka Castel', 'Owens', 'Panama', 'Paris', 'Philly', 'Pikes', 'Pittsburg', 'Po', 'Pompeii', 'Queenstown', 'R', 'Raleigh', 'Raohe', 'Redlands', 'Rehoboth', 'Resort', 'Reston', 'Richmond', 'Rockefeller center', 'Rockville', 'Roma', 'Rome', 'Sake House', 'Salerno', 'San Diego', 'San Francisco', 'San Jose', 'San Luis Obispo', 'Santa Barbra', 'Santa Cruz', 'Sausalito', 'Savannah', 'Seattle', 'Shanghai', 'Shenandoa', 'Shinjuku', 'Singapore', 'Soho', 'Sorimarra', 'Spain', 'St', 'St. Lawrence', 'Sushi', 'Sushi Cho', 'Sydney', 'Taipei', 'Taiwan', 'Taughnanock Falls', 'Tech', 'Teso', 'The', 'The Arc de Triomp', 'Times Square', 'Tokyo', 'Tomo', 'Torg', 'Toronto', 'Toulouse', 'Tyler', 'Tysons', 'US', 'USA', 'V', 'VA', 'Vatican City', 'Venice', 'Villa', 'WA', 'Waffle House', 'Wah Fung', 'Wallops', 'Watkins Glen', 'West End', 'West VA', 'Woodbridge', 'Woodson', 'Xingming', 'Yechon', 'Yi', 'Yi fang', 'Yujin', 'al', 'c', 'g', 'i', 'k', 'le', 's', 'splitsville']\n",
      "Total unique locations: 239\n"
     ]
    }
   ],
   "source": [
    "all_locations = set(\n",
    "    loc\n",
    "    for sublist in df[\"locations\"]\n",
    "    if isinstance(sublist, list)\n",
    "    for loc in sublist\n",
    ")\n",
    "\n",
    "# Convert back to list (if you want)\n",
    "unique_locations = sorted(all_locations)\n",
    "\n",
    "print(unique_locations)\n",
    "print(f\"Total unique locations: {len(unique_locations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
