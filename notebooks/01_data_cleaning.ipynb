{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76990a26",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Parsing, cleaning and structuring the journal data\n",
    "### 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050335f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already available\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words and add custom ones\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = {'got', 'went', 'saw', 'made', 'played', 'home', 'drove', 'day', 'took'}\n",
    "stop_words.update(custom_stop_words) \n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = \"../data/raw/daily_sentences.xlsx\"\n",
    "\n",
    "# Read all sheets as a dictionary of DataFrames\n",
    "sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "df_list = []\n",
    "for sheet_name, df in sheets.items():\n",
    "    df['Date'] = df['Date'].astype(str) + '/' + sheet_name # Adds the year to the date\n",
    "    df_list.append(df)\n",
    "\n",
    "# Combine all sheets into one DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df = df[['Date', 'Sentence']] # Reduce columns\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='raise')\n",
    "df['Sentence'] = df['Sentence'].fillna('')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.to_period('M')\n",
    "df['Day'] = df['Date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939f23d",
   "metadata": {},
   "source": [
    "### 2. Clean the data\n",
    "Check for duplicates/NaNs - I accidently put '02-26' instead of '03-26' and put '11-27' instead of '11-28'\n",
    "\n",
    "NOTE: This would need to be updated and improved if using on a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f199d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 duplicates found...\n",
      "0 duplicates remaining\n",
      "0 null dates found\n"
     ]
    }
   ],
   "source": [
    "# Error checking for duplicate days and entries\n",
    "duplicates = df[df.duplicated('Date', keep=False)]\n",
    "#print(duplicates)\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"{len(duplicates)} duplicates found...\")\n",
    "    # There are two duplicates entires. One has the wrong day and one has the wrong month\n",
    "    df.loc[84, 'Date'] = '2025-03-26'\n",
    "    df.loc[922, 'Date'] = '2021-11-28'\n",
    "    # Recheck for duplicates\n",
    "    duplicates = df[df.duplicated('Date', keep=False)]\n",
    "    print(f\"{len(duplicates)} duplicates remaining\")\n",
    "else:\n",
    "    print(\"No duplicates found\")\n",
    "\n",
    "\n",
    "# Checking for any null dates\n",
    "print(df['Date'].isna().sum(), \"null dates found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba4476",
   "metadata": {},
   "source": [
    "Clean and standardize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16523935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# Function to clean, tokenize, and filter text\n",
    "def tokenize_and_filter(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = pattern.findall(text.lower())\n",
    "    return ' '.join([w for w in words if w not in stop_words])\n",
    "\n",
    "\n",
    "df['cleaned_words'] = df['Sentence'].apply(tokenize_and_filter)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59ca99",
   "metadata": {},
   "source": [
    "Another variation of cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "517960cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def clean_with_spacy(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc \n",
    "              if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['cleaned_words2'] = df['Sentence'].astype(str).apply(clean_with_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded8c85",
   "metadata": {},
   "source": [
    "Output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff2f8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"../data/cleaned/daily_sentences_cleaned.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
